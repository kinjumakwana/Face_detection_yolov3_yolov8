{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc89740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO('yolov8n.pt')\n",
    "\n",
    "# source = 'https://ultralytics.com/images/bus.jpg'\n",
    "# results = model.predict(source, save=True, imgsz=320, conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17352f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Face_detection_Face_recognization'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custome Model \n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO('yolov8n.pt')\n",
    "\n",
    "# # Train the model\n",
    "# results = model.train(data=r'D:/Face Detection_yolov8/data.yaml', epochs=50, imgsz=640, lr0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de865fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO('yolov8n.pt')# load an official model\n",
    "# model = YOLO(r'C:\\Users\\BAPS\\runs\\detect\\train\\weights\\best.pt')  # load a custom model\n",
    "\n",
    "# # Predict with the model\n",
    "# results = model(r'D:\\Face Detection_yolov8\\image1.jpg', save=True, conf=0.5)  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ada2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO('yolov8n.pt')  # load an official model\n",
    "# model = YOLO(r'C:\\Users\\BAPS\\runs\\detect\\train\\weights\\best.pt')  # load a custom model\n",
    "\n",
    "# # Predict with the model\n",
    "# results = model(r'D:\\Face Detection_yolov8\\v1.mp4', save=True, conf=0.5)  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba898813",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model = YOLO(r'C:\\Users\\BAPS\\runs\\detect\\train\\weights\\best.pt')  # load a custom model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Open the video file\u001b[39;00m\n\u001b[0;32m     10\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# loop through video frame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # load an official model\n",
    "# model = YOLO(r'C:\\Users\\BAPS\\runs\\detect\\train\\weights\\best.pt')  # load a custom model\n",
    "\n",
    "# Open the video file\n",
    "video_path = 0\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# loop through video frame\n",
    "while cap.isOpened():\n",
    "    success,frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        result = model(frame,classes=0,conf=0.5)\n",
    "        \n",
    "        annotate_frame = result[0].plot()\n",
    "        \n",
    "        cv2.imshow(\"YOLO8 Interface\",annotate_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03493f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a pretrained YOLOv8n model\n",
    "# model = YOLO('yolov8n.pt')\n",
    "\n",
    "# source=\"rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\"\n",
    "\n",
    "# # Run inference on the source\n",
    "# results = model(source, classes=0, conf=0.4, stream=True)  # generator of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4d1eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=yolov8n.pt classes=0, conf=0.4, source=\"rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f794faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/1: rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif... Success  (inf frames of shape 1920x1080 at 25.00 FPS)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 8.013725280761719, 'inference': 257.6911449432373, 'postprocess': 3.990650177001953}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 7.942438125610352, 'inference': 168.81775856018066, 'postprocess': 2.000570297241211}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.402875900268555, 'inference': 153.9745330810547, 'postprocess': 2.0220279693603516}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.528139114379883, 'inference': 155.3642749786377, 'postprocess': 3.5195350646972656}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.522178649902344, 'inference': 149.95217323303223, 'postprocess': 2.879619598388672}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.677845001220703, 'inference': 146.69489860534668, 'postprocess': 2.4449825286865234}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.536317825317383, 'inference': 166.97001457214355, 'postprocess': 2.905130386352539}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.563901901245117, 'inference': 163.9418601989746, 'postprocess': 2.3851394653320312}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.055261611938477, 'inference': 148.01311492919922, 'postprocess': 1.9490718841552734}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.409313201904297, 'inference': 146.8350887298584, 'postprocess': 2.696990966796875}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.124807357788086, 'inference': 167.85502433776855, 'postprocess': 3.000020980834961}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.523920059204102, 'inference': 145.5252170562744, 'postprocess': 2.908468246459961}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.84101676940918, 'inference': 187.0899200439453, 'postprocess': 2.740144729614258}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.003358840942383, 'inference': 154.78038787841797, 'postprocess': 4.2285919189453125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 7.492542266845703, 'inference': 157.01007843017578, 'postprocess': 2.056598663330078}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 0.0, 'inference': 175.92501640319824, 'postprocess': 3.013134002685547}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 0.0, 'inference': 165.53163528442383, 'postprocess': 10.111570358276367}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 0.0, 'inference': 180.30762672424316, 'postprocess': 2.9718875885009766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 9.628057479858398, 'inference': 152.55999565124512, 'postprocess': 7.475376129150391}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.008769989013672, 'inference': 140.86437225341797, 'postprocess': 1.9903182983398438}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.008293151855469, 'inference': 147.74322509765625, 'postprocess': 2.0003318786621094}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.51605224609375, 'inference': 174.21674728393555, 'postprocess': 3.2792091369628906}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.242420196533203, 'inference': 172.37138748168945, 'postprocess': 2.002716064453125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.958391189575195, 'inference': 183.27808380126953, 'postprocess': 3.5190582275390625}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.515575408935547, 'inference': 166.029691696167, 'postprocess': 2.277374267578125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.521297454833984, 'inference': 172.77193069458008, 'postprocess': 2.530813217163086}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.968477249145508, 'inference': 151.35884284973145, 'postprocess': 2.5625228881835938}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.9992332458496094, 'inference': 153.32603454589844, 'postprocess': 3.0002593994140625}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 7.59124755859375, 'inference': 174.3452548980713, 'postprocess': 3.1130313873291016}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.084442138671875, 'inference': 142.96412467956543, 'postprocess': 3.000497817993164}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.977060317993164, 'inference': 158.6289405822754, 'postprocess': 1.9674301147460938}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.999471664428711, 'inference': 166.44573211669922, 'postprocess': 1.9888877868652344}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.0083656311035156, 'inference': 157.79542922973633, 'postprocess': 3.511667251586914}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.098819732666016, 'inference': 156.58092498779297, 'postprocess': 3.002643585205078}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.5071372985839844, 'inference': 156.74066543579102, 'postprocess': 3.7088394165039062}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.523370742797852, 'inference': 166.48173332214355, 'postprocess': 2.5148391723632812}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.3457489013671875, 'inference': 172.82867431640625, 'postprocess': 3.006458282470703}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.135297775268555, 'inference': 137.04156875610352, 'postprocess': 2.117156982421875}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.13713264465332, 'inference': 154.00314331054688, 'postprocess': 2.9413700103759766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.999471664428711, 'inference': 169.69919204711914, 'postprocess': 2.897500991821289}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.307674407958984, 'inference': 136.2764835357666, 'postprocess': 1.9979476928710938}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.1189918518066406, 'inference': 153.46360206604004, 'postprocess': 2.039670944213867}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.028797149658203, 'inference': 165.03262519836426, 'postprocess': 4.013299942016602}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.527019500732422, 'inference': 147.05610275268555, 'postprocess': 3.0024051666259766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.999948501586914, 'inference': 147.35841751098633, 'postprocess': 1.0106563568115234}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.004789352416992, 'inference': 154.88839149475098, 'postprocess': 2.9993057250976562}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.002021789550781, 'inference': 158.04123878479004, 'postprocess': 2.519369125366211}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.448268890380859, 'inference': 155.46512603759766, 'postprocess': 2.009153366088867}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.436969757080078, 'inference': 154.8609733581543, 'postprocess': 3.000497817993164}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.347085952758789, 'inference': 145.3537940979004, 'postprocess': 3.0019283294677734}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.536794662475586, 'inference': 148.3595371246338, 'postprocess': 3.0307769775390625}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.779193878173828, 'inference': 165.0848388671875, 'postprocess': 2.000093460083008}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.0007362365722656, 'inference': 170.61734199523926, 'postprocess': 2.5191307067871094}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.212856292724609, 'inference': 145.36571502685547, 'postprocess': 2.5908946990966797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.349470138549805, 'inference': 150.9854793548584, 'postprocess': 2.0873546600341797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.486322402954102, 'inference': 158.25152397155762, 'postprocess': 1.7554759979248047}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.044221878051758, 'inference': 148.28085899353027, 'postprocess': 2.0110607147216797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.767417907714844, 'inference': 144.3936824798584, 'postprocess': 2.001047134399414}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.999471664428711, 'inference': 162.57739067077637, 'postprocess': 2.3162364959716797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 9.74130630493164, 'inference': 158.2012176513672, 'postprocess': 3.7539005279541016}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.00178337097168, 'inference': 147.82452583312988, 'postprocess': 3.5920143127441406}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.009889602661133, 'inference': 150.70319175720215, 'postprocess': 1.9958019256591797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 2.9675960540771484, 'inference': 149.1873264312744, 'postprocess': 2.001523971557617}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.519390106201172, 'inference': 159.96074676513672, 'postprocess': 2.0055770874023438}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.521369934082031, 'inference': 144.58441734313965, 'postprocess': 2.000570297241211}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.512144088745117, 'inference': 160.9363555908203, 'postprocess': 3.9987564086914062}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.003047943115234, 'inference': 147.53365516662598, 'postprocess': 2.0351409912109375}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.00090217590332, 'inference': 137.4499797821045, 'postprocess': 3.60870361328125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.318807601928711, 'inference': 151.09896659851074, 'postprocess': 3.0562877655029297}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.0388832092285156, 'inference': 138.75269889831543, 'postprocess': 1.9583702087402344}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.180431365966797, 'inference': 170.60232162475586, 'postprocess': 2.610445022583008}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.370782852172852, 'inference': 148.65469932556152, 'postprocess': 2.6786327362060547}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.000974655151367, 'inference': 164.96944427490234, 'postprocess': 2.0360946655273438}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.148244857788086, 'inference': 157.55176544189453, 'postprocess': 3.201007843017578}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.009723663330078, 'inference': 150.68578720092773, 'postprocess': 3.2405853271484375}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.565881729125977, 'inference': 150.55036544799805, 'postprocess': 2.6197433471679688}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.338907241821289, 'inference': 171.64182662963867, 'postprocess': 3.0024051666259766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.50897216796875, 'inference': 139.41407203674316, 'postprocess': 2.0020008087158203}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.5238265991210938, 'inference': 162.1413230895996, 'postprocess': 2.5179386138916016}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.001544952392578, 'inference': 143.80359649658203, 'postprocess': 1.9989013671875}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.236553192138672, 'inference': 159.1358184814453, 'postprocess': 2.5534629821777344}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.352569580078125, 'inference': 170.7327365875244, 'postprocess': 7.9059600830078125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.5300254821777344, 'inference': 145.28703689575195, 'postprocess': 3.6323070526123047}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.041433334350586, 'inference': 157.09853172302246, 'postprocess': 0.9989738464355469}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.09278678894043, 'inference': 151.1557102203369, 'postprocess': 3.0012130737304688}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 7.5130462646484375, 'inference': 154.55341339111328, 'postprocess': 2.0024776458740234}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.631757736206055, 'inference': 143.67961883544922, 'postprocess': 2.338409423828125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.5528411865234375, 'inference': 157.32765197753906, 'postprocess': 2.5682449340820312}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 7.033109664916992, 'inference': 156.99315071105957, 'postprocess': 4.126787185668945}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.513357162475586, 'inference': 134.7806453704834, 'postprocess': 2.0432472229003906}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.002571105957031, 'inference': 153.99765968322754, 'postprocess': 1.9872188568115234}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.588438034057617, 'inference': 153.1665325164795, 'postprocess': 3.042459487915039}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.005359649658203, 'inference': 144.80876922607422, 'postprocess': 1.6293525695800781}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.06846809387207, 'inference': 167.01698303222656, 'postprocess': 1.8839836120605469}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 8.172988891601562, 'inference': 163.49482536315918, 'postprocess': 2.5222301483154297}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.7149658203125, 'inference': 143.7985897064209, 'postprocess': 2.012968063354492}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.6623687744140625, 'inference': 149.13296699523926, 'postprocess': 3.093242645263672}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.5469532012939453, 'inference': 148.37169647216797, 'postprocess': 3.0329227447509766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.506038665771484, 'inference': 149.25670623779297, 'postprocess': 2.9916763305664062}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.964040756225586, 'inference': 167.8919792175293, 'postprocess': 2.2439956665039062}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.978511810302734, 'inference': 172.24884033203125, 'postprocess': 3.5941600799560547}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.295515060424805, 'inference': 163.71989250183105, 'postprocess': 2.596139907836914}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.010532379150391, 'inference': 165.0080680847168, 'postprocess': 2.003192901611328}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.024505615234375, 'inference': 160.5851650238037, 'postprocess': 1.5799999237060547}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.997491836547852, 'inference': 153.95736694335938, 'postprocess': 3.0815601348876953}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 9.321451187133789, 'inference': 179.55279350280762, 'postprocess': 2.999544143676758}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.301786422729492, 'inference': 177.8271198272705, 'postprocess': 2.919912338256836}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.006385803222656, 'inference': 156.9828987121582, 'postprocess': 1.007080078125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.516767501831055, 'inference': 138.5636329650879, 'postprocess': 1.9989013671875}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 6.403684616088867, 'inference': 139.51826095581055, 'postprocess': 3.3843517303466797}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.569768905639648, 'inference': 169.95477676391602, 'postprocess': 2.835988998413086}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.504846572875977, 'inference': 151.43775939941406, 'postprocess': 2.9430389404296875}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.510713577270508, 'inference': 173.29955101013184, 'postprocess': 2.381563186645508}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.000497817993164, 'inference': 148.0543613433838, 'postprocess': 4.5185089111328125}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 0.0, 'inference': 161.93437576293945, 'postprocess': 2.9668807983398438}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 4.334211349487305, 'inference': 134.8247528076172, 'postprocess': 2.655506134033203}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 3.9627552032470703, 'inference': 148.7441062927246, 'postprocess': 2.9261112213134766}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.595207214355469, 'inference': 144.9747085571289, 'postprocess': 3.0062198638916016}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.0373077392578125, 'inference': 153.7637710571289, 'postprocess': 3.0002593994140625}\n",
      "\n",
      "\n",
      "rtsp_//admin_L2AFBE6F_192.168.29.112_554/cam/realmonitor_channel_1_subtype_0_unicast_true_proto_Onvif\n",
      "{'preprocess': 5.000591278076172, 'inference': 160.67814826965332, 'postprocess': 2.001047134399414}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use the model to detect object\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model.predict(source=video_path,classes=0,conf=0.4,stream=True,save=True,verbose=False)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m model(video_path, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stream_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mspeed)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:253\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 253\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:133\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    131\u001b[0m     visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[0;32m    132\u001b[0m                                mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:339\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    336\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    341\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 82\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m     83\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:47\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m shape \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# BCHW\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[1;32m---> 47\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2[i](x[i]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:42\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    " \n",
    "# Load custom trained YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "video_path = \"rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\"\n",
    "\n",
    "# Use the model to detect object\n",
    "# model.predict(source=video_path,classes=0,conf=0.4,stream=True,save=True,verbose=False)\n",
    "\n",
    "results = model(video_path, stream=True, classes=0,conf=0.4, save=True, verbose=False, stream_buffer=True)\n",
    "\n",
    "for result in results:\n",
    "    print(result.path)\n",
    "    print(result.speed)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3014968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import time\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load the YOLOv8 model\n",
    "# model = YOLO('yolov8n.pt')  # load an official model\n",
    "\n",
    "# video_path = \"rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # loop through video frame\n",
    "# while cap.isOpened():\n",
    "#     success,frame = cap.read()\n",
    "        \n",
    "#     if success:\n",
    "#         result = model.predict(frame, classes=0,conf=0.5, stream=True)\n",
    "        \n",
    "#         annotate_frame = result[0].plot()\n",
    "        \n",
    "#         cv2.imshow(\"YOLO8 Interface\",annotate_frame)\n",
    "        \n",
    "#         if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# out.release()  # Release the video writer\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fc2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 81.4ms\n",
      "Speed: 2.6ms preprocess, 81.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 111.4ms\n",
      "Speed: 4.4ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.7ms\n",
      "Speed: 3.0ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 71.1ms\n",
      "Speed: 5.3ms preprocess, 71.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 86.8ms\n",
      "Speed: 8.4ms preprocess, 86.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 90.4ms\n",
      "Speed: 2.6ms preprocess, 90.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 91.1ms\n",
      "Speed: 2.6ms preprocess, 91.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.3ms\n",
      "Speed: 4.6ms preprocess, 102.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 92.1ms\n",
      "Speed: 0.0ms preprocess, 92.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.2ms\n",
      "Speed: 8.8ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.6ms\n",
      "Speed: 9.6ms preprocess, 88.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 111.6ms\n",
      "Speed: 0.0ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.9ms\n",
      "Speed: 6.1ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.3ms\n",
      "Speed: 8.4ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 87.3ms\n",
      "Speed: 4.9ms preprocess, 87.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.3ms\n",
      "Speed: 4.1ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 85.7ms\n",
      "Speed: 9.7ms preprocess, 85.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 3.6ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 6.8ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.7ms\n",
      "Speed: 5.6ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.3ms\n",
      "Speed: 10.5ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.7ms\n",
      "Speed: 3.0ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.7ms\n",
      "Speed: 4.8ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.4ms\n",
      "Speed: 9.8ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.2ms\n",
      "Speed: 8.9ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.1ms\n",
      "Speed: 10.2ms preprocess, 118.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 87.6ms\n",
      "Speed: 12.3ms preprocess, 87.6ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.4ms\n",
      "Speed: 6.7ms preprocess, 89.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.5ms\n",
      "Speed: 10.1ms preprocess, 102.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.9ms\n",
      "Speed: 5.1ms preprocess, 101.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.8ms\n",
      "Speed: 9.8ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.8ms\n",
      "Speed: 3.8ms preprocess, 88.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 10.0ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.1ms\n",
      "Speed: 9.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.1ms\n",
      "Speed: 5.5ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.8ms\n",
      "Speed: 9.1ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.3ms\n",
      "Speed: 9.1ms preprocess, 88.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.5ms\n",
      "Speed: 7.7ms preprocess, 120.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.9ms\n",
      "Speed: 5.9ms preprocess, 101.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.2ms\n",
      "Speed: 9.3ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 8.1ms preprocess, 103.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 6.1ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.4ms\n",
      "Speed: 9.0ms preprocess, 103.4ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.4ms\n",
      "Speed: 9.1ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 94.9ms\n",
      "Speed: 0.0ms preprocess, 94.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.0ms\n",
      "Speed: 7.9ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.9ms\n",
      "Speed: 6.5ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.6ms\n",
      "Speed: 7.1ms preprocess, 88.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.1ms\n",
      "Speed: 6.6ms preprocess, 106.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.2ms\n",
      "Speed: 8.3ms preprocess, 89.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.6ms\n",
      "Speed: 6.4ms preprocess, 88.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.1ms\n",
      "Speed: 9.2ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 8.8ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 74.1ms\n",
      "Speed: 2.8ms preprocess, 74.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 7.3ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 112.4ms\n",
      "Speed: 0.0ms preprocess, 112.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 99.2ms\n",
      "Speed: 0.0ms preprocess, 99.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.4ms\n",
      "Speed: 6.6ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.8ms\n",
      "Speed: 8.1ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.7ms\n",
      "Speed: 8.7ms preprocess, 118.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.4ms\n",
      "Speed: 10.3ms preprocess, 88.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.7ms\n",
      "Speed: 3.2ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.2ms\n",
      "Speed: 8.6ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 106.7ms\n",
      "Speed: 6.5ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 72.4ms\n",
      "Speed: 9.5ms preprocess, 72.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.9ms\n",
      "Speed: 8.6ms preprocess, 106.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.6ms\n",
      "Speed: 5.9ms preprocess, 105.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 8.2ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 90.7ms\n",
      "Speed: 11.1ms preprocess, 90.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 103.3ms\n",
      "Speed: 8.3ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.1ms\n",
      "Speed: 3.2ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.5ms\n",
      "Speed: 3.6ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.8ms\n",
      "Speed: 8.1ms preprocess, 89.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.2ms\n",
      "Speed: 9.8ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.4ms\n",
      "Speed: 9.6ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.5ms\n",
      "Speed: 8.5ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.6ms\n",
      "Speed: 3.4ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 74.3ms\n",
      "Speed: 5.2ms preprocess, 74.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 110.8ms\n",
      "Speed: 0.0ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.7ms\n",
      "Speed: 9.1ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 10.1ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.9ms\n",
      "Speed: 9.9ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 99.0ms\n",
      "Speed: 0.0ms preprocess, 99.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.4ms\n",
      "Speed: 8.1ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.9ms\n",
      "Speed: 6.7ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 112.9ms\n",
      "Speed: 0.0ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.4ms\n",
      "Speed: 7.6ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 74.0ms\n",
      "Speed: 3.6ms preprocess, 74.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 84.5ms\n",
      "Speed: 9.1ms preprocess, 84.5ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 87.0ms\n",
      "Speed: 7.7ms preprocess, 87.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.4ms\n",
      "Speed: 7.1ms preprocess, 88.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 10.0ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.3ms\n",
      "Speed: 5.6ms preprocess, 88.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 114.3ms\n",
      "Speed: 8.0ms preprocess, 114.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 7.7ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 73.5ms\n",
      "Speed: 6.5ms preprocess, 73.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.3ms\n",
      "Speed: 7.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.8ms\n",
      "Speed: 9.6ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.5ms\n",
      "Speed: 6.4ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.0ms\n",
      "Speed: 10.1ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 7.2ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.9ms\n",
      "Speed: 8.0ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 91.0ms\n",
      "Speed: 2.7ms preprocess, 91.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.5ms\n",
      "Speed: 9.9ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.5ms\n",
      "Speed: 2.5ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.0ms\n",
      "Speed: 5.8ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.6ms\n",
      "Speed: 6.7ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.4ms\n",
      "Speed: 6.4ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 8.4ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.7ms\n",
      "Speed: 9.7ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 103.5ms\n",
      "Speed: 7.2ms preprocess, 103.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 104.2ms\n",
      "Speed: 7.0ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 103.6ms\n",
      "Speed: 8.6ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 105.3ms\n",
      "Speed: 5.2ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 104.7ms\n",
      "Speed: 7.3ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 88.9ms\n",
      "Speed: 7.8ms preprocess, 88.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 108.2ms\n",
      "Speed: 0.0ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 119.0ms\n",
      "Speed: 7.5ms preprocess, 119.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 98.4ms\n",
      "Speed: 9.9ms preprocess, 98.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 103.7ms\n",
      "Speed: 3.3ms preprocess, 103.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 107.6ms\n",
      "Speed: 0.0ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.6ms\n",
      "Speed: 10.6ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.1ms\n",
      "Speed: 8.1ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.7ms\n",
      "Speed: 7.0ms preprocess, 102.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.4ms\n",
      "Speed: 2.5ms preprocess, 118.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 112.1ms\n",
      "Speed: 0.0ms preprocess, 112.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.2ms\n",
      "Speed: 7.2ms preprocess, 120.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 103.7ms\n",
      "Speed: 9.6ms preprocess, 103.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.8ms\n",
      "Speed: 7.2ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.7ms\n",
      "Speed: 3.9ms preprocess, 109.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 73.1ms\n",
      "Speed: 8.1ms preprocess, 73.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 7.9ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.4ms\n",
      "Speed: 7.6ms preprocess, 89.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.1ms\n",
      "Speed: 5.0ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.0ms\n",
      "Speed: 5.3ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.7ms\n",
      "Speed: 5.4ms preprocess, 89.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 97.0ms\n",
      "Speed: 0.0ms preprocess, 97.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 9.6ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 101.6ms\n",
      "Speed: 9.5ms preprocess, 101.6ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 91.5ms\n",
      "Speed: 9.6ms preprocess, 91.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.0ms\n",
      "Speed: 3.7ms preprocess, 102.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.8ms\n",
      "Speed: 3.0ms preprocess, 107.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.3ms\n",
      "Speed: 6.2ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 8.9ms preprocess, 105.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.6ms\n",
      "Speed: 4.2ms preprocess, 118.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 97.5ms\n",
      "Speed: 14.0ms preprocess, 97.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.4ms\n",
      "Speed: 6.8ms preprocess, 89.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.0ms\n",
      "Speed: 8.1ms preprocess, 102.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.2ms\n",
      "Speed: 8.0ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.0ms\n",
      "Speed: 9.6ms preprocess, 119.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.6ms\n",
      "Speed: 9.9ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.9ms\n",
      "Speed: 7.5ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.5ms\n",
      "Speed: 8.4ms preprocess, 118.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 121.9ms\n",
      "Speed: 7.6ms preprocess, 121.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.0ms\n",
      "Speed: 0.0ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.2ms\n",
      "Speed: 9.9ms preprocess, 118.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.7ms\n",
      "Speed: 8.2ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.2ms\n",
      "Speed: 7.3ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 3.0ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.1ms\n",
      "Speed: 8.5ms preprocess, 120.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.3ms\n",
      "Speed: 11.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.6ms\n",
      "Speed: 7.7ms preprocess, 107.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 114.9ms\n",
      "Speed: 0.0ms preprocess, 114.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.8ms\n",
      "Speed: 3.0ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 87.0ms\n",
      "Speed: 12.1ms preprocess, 87.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.5ms\n",
      "Speed: 6.8ms preprocess, 89.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 94.4ms\n",
      "Speed: 3.5ms preprocess, 94.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.6ms\n",
      "Speed: 9.6ms preprocess, 116.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.5ms\n",
      "Speed: 8.6ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 122.4ms\n",
      "Speed: 0.0ms preprocess, 122.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.7ms\n",
      "Speed: 0.0ms preprocess, 119.7ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 1.1ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.7ms\n",
      "Speed: 6.5ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 123.0ms\n",
      "Speed: 16.1ms preprocess, 123.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.7ms\n",
      "Speed: 10.3ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 111.2ms\n",
      "Speed: 4.4ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 158.8ms\n",
      "Speed: 0.0ms preprocess, 158.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.4ms\n",
      "Speed: 10.0ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.9ms\n",
      "Speed: 9.5ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.3ms\n",
      "Speed: 2.6ms preprocess, 115.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 91.4ms\n",
      "Speed: 5.8ms preprocess, 91.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.9ms\n",
      "Speed: 6.1ms preprocess, 115.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 124.1ms\n",
      "Speed: 5.5ms preprocess, 124.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.5ms\n",
      "Speed: 8.6ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.1ms\n",
      "Speed: 7.7ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 90.2ms\n",
      "Speed: 7.2ms preprocess, 90.2ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.2ms\n",
      "Speed: 8.5ms preprocess, 120.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 113.1ms\n",
      "Speed: 0.0ms preprocess, 113.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 136.5ms\n",
      "Speed: 3.6ms preprocess, 136.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 123.4ms\n",
      "Speed: 6.5ms preprocess, 123.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 118.5ms\n",
      "Speed: 10.7ms preprocess, 118.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 108.8ms\n",
      "Speed: 8.4ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 124.9ms\n",
      "Speed: 8.2ms preprocess, 124.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.7ms\n",
      "Speed: 10.6ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.7ms\n",
      "Speed: 3.6ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 121.4ms\n",
      "Speed: 5.7ms preprocess, 121.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 138.8ms\n",
      "Speed: 9.6ms preprocess, 138.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.3ms\n",
      "Speed: 2.8ms preprocess, 116.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 139.5ms\n",
      "Speed: 2.6ms preprocess, 139.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.5ms\n",
      "Speed: 3.4ms preprocess, 107.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 125.1ms\n",
      "Speed: 7.8ms preprocess, 125.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 111.9ms\n",
      "Speed: 6.4ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.5ms\n",
      "Speed: 7.0ms preprocess, 88.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 108.4ms\n",
      "Speed: 9.1ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.6ms\n",
      "Speed: 9.1ms preprocess, 119.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 112.4ms\n",
      "Speed: 2.7ms preprocess, 112.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 122.1ms\n",
      "Speed: 8.6ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 107.2ms\n",
      "Speed: 3.1ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.2ms\n",
      "Speed: 7.8ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.8ms\n",
      "Speed: 4.5ms preprocess, 105.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.9ms\n",
      "Speed: 10.0ms preprocess, 120.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.6ms\n",
      "Speed: 8.8ms preprocess, 118.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 152.1ms\n",
      "Speed: 8.4ms preprocess, 152.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 148.8ms\n",
      "Speed: 10.5ms preprocess, 148.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 160.6ms\n",
      "Speed: 5.2ms preprocess, 160.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 165.4ms\n",
      "Speed: 3.3ms preprocess, 165.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 151.4ms\n",
      "Speed: 10.7ms preprocess, 151.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 145.1ms\n",
      "Speed: 3.8ms preprocess, 145.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 121.7ms\n",
      "Speed: 3.6ms preprocess, 121.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 142.0ms\n",
      "Speed: 9.2ms preprocess, 142.0ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 190.2ms\n",
      "Speed: 11.4ms preprocess, 190.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 146.9ms\n",
      "Speed: 4.5ms preprocess, 146.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 153.0ms\n",
      "Speed: 4.6ms preprocess, 153.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 152.3ms\n",
      "Speed: 2.8ms preprocess, 152.3ms inference, 16.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 157.5ms\n",
      "Speed: 15.8ms preprocess, 157.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 114.0ms\n",
      "Speed: 2.8ms preprocess, 114.0ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 124.8ms\n",
      "Speed: 5.6ms preprocess, 124.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 121.4ms\n",
      "Speed: 8.0ms preprocess, 121.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.5ms\n",
      "Speed: 9.9ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.6ms\n",
      "Speed: 10.7ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.5ms\n",
      "Speed: 3.1ms preprocess, 105.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.9ms\n",
      "Speed: 3.7ms preprocess, 119.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 97.7ms\n",
      "Speed: 5.7ms preprocess, 97.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 162.4ms\n",
      "Speed: 6.7ms preprocess, 162.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 162.1ms\n",
      "Speed: 13.0ms preprocess, 162.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 155.6ms\n",
      "Speed: 4.4ms preprocess, 155.6ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 171.5ms\n",
      "Speed: 0.0ms preprocess, 171.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 150.2ms\n",
      "Speed: 13.0ms preprocess, 150.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 158.2ms\n",
      "Speed: 2.5ms preprocess, 158.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 167.8ms\n",
      "Speed: 13.3ms preprocess, 167.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 149.5ms\n",
      "Speed: 5.9ms preprocess, 149.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 156.6ms\n",
      "Speed: 3.7ms preprocess, 156.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 152.4ms\n",
      "Speed: 7.0ms preprocess, 152.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 166.6ms\n",
      "Speed: 0.0ms preprocess, 166.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.9ms\n",
      "Speed: 13.3ms preprocess, 117.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 114.3ms\n",
      "Speed: 3.8ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 105.5ms\n",
      "Speed: 3.5ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.3ms\n",
      "Speed: 3.8ms preprocess, 120.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.5ms\n",
      "Speed: 6.4ms preprocess, 103.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.9ms\n",
      "Speed: 3.1ms preprocess, 116.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 151.5ms\n",
      "Speed: 5.9ms preprocess, 151.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.5ms\n",
      "Speed: 9.9ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.1ms\n",
      "Speed: 10.5ms preprocess, 120.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.2ms\n",
      "Speed: 3.4ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.4ms\n",
      "Speed: 10.4ms preprocess, 103.4ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 103.6ms\n",
      "Speed: 3.7ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.0ms\n",
      "Speed: 3.6ms preprocess, 115.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.5ms\n",
      "Speed: 3.6ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.3ms\n",
      "Speed: 5.3ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.3ms\n",
      "Speed: 7.4ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.0ms\n",
      "Speed: 9.1ms preprocess, 88.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 118.4ms\n",
      "Speed: 3.7ms preprocess, 118.4ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.8ms\n",
      "Speed: 6.9ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.9ms\n",
      "Speed: 5.8ms preprocess, 119.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 124.8ms\n",
      "Speed: 5.3ms preprocess, 124.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 106.7ms\n",
      "Speed: 2.3ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 114.0ms\n",
      "Speed: 2.1ms preprocess, 114.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 91.5ms\n",
      "Speed: 6.2ms preprocess, 91.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 104.9ms\n",
      "Speed: 7.4ms preprocess, 104.9ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 107.4ms\n",
      "Speed: 4.3ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 123.0ms\n",
      "Speed: 3.2ms preprocess, 123.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.7ms\n",
      "Speed: 6.9ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 76.4ms\n",
      "Speed: 2.3ms preprocess, 76.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 106.7ms\n",
      "Speed: 8.2ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 108.6ms\n",
      "Speed: 2.5ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 88.6ms\n",
      "Speed: 8.1ms preprocess, 88.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 108.0ms\n",
      "Speed: 4.2ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 87.8ms\n",
      "Speed: 8.5ms preprocess, 87.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.2ms\n",
      "Speed: 4.2ms preprocess, 119.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.5ms\n",
      "Speed: 7.5ms preprocess, 115.5ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.8ms\n",
      "Speed: 8.1ms preprocess, 109.8ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 129.9ms\n",
      "Speed: 6.3ms preprocess, 129.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.1ms\n",
      "Speed: 3.1ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 168.6ms\n",
      "Speed: 10.6ms preprocess, 168.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 132.1ms\n",
      "Speed: 3.8ms preprocess, 132.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 132.2ms\n",
      "Speed: 4.3ms preprocess, 132.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 126.6ms\n",
      "Speed: 9.2ms preprocess, 126.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.6ms\n",
      "Speed: 4.0ms preprocess, 109.6ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 113.2ms\n",
      "Speed: 2.8ms preprocess, 113.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 122.1ms\n",
      "Speed: 3.7ms preprocess, 122.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 125.3ms\n",
      "Speed: 4.9ms preprocess, 125.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 124.7ms\n",
      "Speed: 3.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 110.7ms\n",
      "Speed: 0.0ms preprocess, 110.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 121.6ms\n",
      "Speed: 3.4ms preprocess, 121.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 2.9ms preprocess, 110.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 117.0ms\n",
      "Speed: 3.5ms preprocess, 117.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m---> 37\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(frame, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m     39\u001b[0m     annotate_frame \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n\u001b[0;32m     41\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(cv2\u001b[38;5;241m.\u001b[39mcvtColor(annotate_frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\model.py:95\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\model.py:235\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:194\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:253\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 253\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:133\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    131\u001b[0m     visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[0;32m    132\u001b[0m                                mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:339\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    336\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    341\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 82\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m     83\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:42\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\functional.py:2058\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   2059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # load an official model\n",
    "\n",
    "video_reader = \"rtsp://admin:L2AFBE6F@192.168.29.112:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\"\n",
    "\n",
    "# video_path = \"path/to/video.mp4\"\n",
    "cap = cv2.VideoCapture(video_reader)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Video', cv2.WINDOW_NORMAL)\n",
    " \n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Desired smaller frame dimensions\n",
    "desired_width = 640  # Adjust this value as needed 640/1280\n",
    "desired_height = 384  # Adjust this value as needed 384/720\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Use MJPG codec for stream format\n",
    "out = cv2.VideoWriter('output_office_webcam.avi', fourcc,fps,(frame_width, frame_height))  # Save as AVI file\n",
    "\n",
    "# loop through video frame\n",
    "while cap.isOpened():\n",
    "    success,frame = cap.read()\n",
    "        \n",
    "    # Check if frame is not read properly then break the loop\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    if success:\n",
    "        result = model(frame, classes=0,conf=0.4)\n",
    "        \n",
    "        annotate_frame = result[0].plot()\n",
    "        \n",
    "        out.write(cv2.cvtColor(annotate_frame, cv2.COLOR_RGB2BGR))\n",
    "        out.write(annotate_frame)\n",
    "        \n",
    "        # Resize the result frame to match the original frame size\n",
    "        annotate_frame = cv2.resize(annotate_frame, (desired_width, desired_height))\n",
    "\n",
    "        cv2.imshow(\"YOLO8 Interface\",annotate_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "out.release()  # Release the video writer\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a707d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
